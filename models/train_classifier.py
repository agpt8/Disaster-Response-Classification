import pickle
import sys

import nltk
import numpy as np
import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from scipy.stats.mstats import gmean
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import AdaBoostClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.metrics import classification_report, fbeta_score, make_scorer
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.multioutput import MultiOutputClassifier
from sklearn.pipeline import FeatureUnion, Pipeline
from sqlalchemy import create_engine

nltk.download(['punkt', 'wordnet'])
nltk.download('averaged_perceptron_tagger')


def load_data(database_filepath):
    """
    Load Data Function:
        1. Read input SQLite database generated by ETL
        2. Split dataframe into Feature & Label
    
    Args:
        database_filepath (str): SQLite database file path

    Returns:
        Feature Dataframe, Label Dataframe, category_names used for data
        visualization in the app
    """
    engine = create_engine('sqlite:///' + database_filepath)
    df = pd.read_sql_table('message_categories', engine)
    x = df['message']
    y = df.iloc[:, 4:]
    category_names = y.columns
    return x, y, category_names


def tokenize(text):
    """
    Tokenize function to process text data including lemmatize, normalize
    case, and remove leading/trailing white space
    
    Args:
        text (str): list of text messages (english)

    Returns:
        clean_tokens: tokenized text, clean and ready to feed ML modeling
    """
    tokens = word_tokenize(text)
    lemmatizer = WordNetLemmatizer()

    clean_tokens = []
    for token in tokens:
        clean_token = lemmatizer.lemmatize(token).lower().strip()
        clean_tokens.append(clean_token)

    return clean_tokens


class StartingVerbExtractor(BaseEstimator, TransformerMixin):
    """
    This class extract the starting verb of a sentence,
    creating a new feature for the ML classifier
    """

    def isverb(self, text):
        """
        Check if the starting word is a verb
        Args:
            text (str): text messages to be checked

        Returns:
            boolean (bool)
        """
        sentence_list = nltk.sent_tokenize(text)
        for sentence in sentence_list:
            pos_tags = nltk.pos_tag(tokenize(sentence))
            first_word, first_tag = pos_tags[0]
            if first_tag in ['VB', 'VBP'] or first_word == 'RT':
                return True
        return False

    def fit(self, x, y=None):
        return self

    def transform(self, x):
        """
          Transform incoming dataframe to check for starting verb
           Args:
              x:
           Returns:
              dataframe with tagged verbs (pd.Dataframe)
          """
        x_tagged = pd.Series(x).apply(self.isverb)
        return pd.DataFrame(x_tagged)


def multioutput_fscore(y_true, y_pred, beta=1):
    """
    This is a performance metric reference.
    It is a sort of geometric mean of the fbeta_score, computed on each label.
    It is compatible with multi-label and multi-class problems.
    It features some peculiarities (geometric mean, 100% removal...) to exclude
    trivial solutions and deliberately under-estimate a standard fbeta_score average.
    The aim is avoiding issues when dealing with multi-class/multi-label imbalanced cases.
    It can be used as scorer for GridSearchCV:
        scorer = make_scorer(multioutput_fscore, beta=1)
        
    Args:
        y_true: labels
        y_pred: predictions
        beta: beta value of fscore metric
    
    Returns:
        customized fscore
    """
    score_list = []
    if isinstance(y_pred, pd.DataFrame):
        y_pred = y_pred.values
    if isinstance(y_true, pd.DataFrame):
        y_true = y_true.values
    for column in range(0, y_true.shape[1]):
        score = fbeta_score(y_true[:, column], y_pred[:, column], beta,
                            average='weighted')
        score_list.append(score)
    f1score_numpy = np.asarray(score_list)
    f1score_numpy = f1score_numpy[f1score_numpy < 1]
    f1score = gmean(f1score_numpy)
    return f1score


def build_model():
    """
    This function returns a ML Pipeline that process text messages
    according to NLP best-practice and apply a classifier.
    """
    pipeline = Pipeline([('features', FeatureUnion([('text_pipeline', Pipeline(
        [('vect', CountVectorizer(tokenizer=tokenize)),
         ('tfidf', TfidfTransformer()),
         ])), ('isverb', StartingVerbExtractor())])),
                         ('clf', MultiOutputClassifier(AdaBoostClassifier()))])

    # Use grid search to find better parameters
    parameters = {
        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),
        'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),
        'features__text_pipeline__vect__max_features': (None, 5000, 10000),
        'features__text_pipeline__tfidf__use_idf': (True, False),
    }

    scorer = make_scorer(multioutput_fscore, greater_is_better=True)
    cv = GridSearchCV(pipeline, param_grid=parameters, scoring=scorer,
                      verbose=2, n_jobs=-1)
    return cv


def evaluate_model(model, x_test, y_test, category_names):
    """
    This function use the provided ML pipeline to predict on a test set
    and report the f1 score, precision and recall for each output category of the dataset
    
    Args:
        model: ML Pipeline
        x_test: test features
        y_test: test labels
        category_names: label names (multi-output)
    """
    y_pred = model.predict(x_test)

    for i in range(len(category_names)):
        print('\n')
        print(f'Category: {category_names[i]}%\n{classification_report(y_test.iloc[:, i], y_pred[:, i])}')

    multioutput_f1 = multioutput_fscore(y_test, y_pred)
    overall_accuracy = (y_pred == y_test).mean().mean()
    print(f'Average overall accuracy: {overall_accuracy * 100:.2f}%\n')
    print(f'F1 score (custom definition): {multioutput_f1 * 100:.2f}%\n')


def save_model(model, model_filepath):
    """
    This function saves trained model as Pickle file, to be loaded later.

    Args:
        model: GridSearchCV or Scikit Pipeline object
        model_filepath (str): output pickle file path & name
    """
    pickle.dump(model, open(model_filepath, 'wb'))


def main():
    """
    This function applies the Machine Learning Pipeline:
        1) Extract data from SQLite db
        2) Train ML model on training set
        3) Estimate model performance on test set
        4) Save trained model as Pickle
    """
    if len(sys.argv) == 3:
        database_filepath, model_filepath = sys.argv[1:]
        print(f'Loading data...\n    DATABASE: {database_filepath}')
        x, y, category_names = load_data(database_filepath)
        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

        print('Building model...')
        model = build_model()

        print('Training model...')
        model.fit(x_train, y_train)

        print('Evaluating model...')
        evaluate_model(model, x_test, y_test, category_names)

        print(f'Saving model...\n    MODEL: {model_filepath}')
        save_model(model, model_filepath)

        print('Trained model saved!')

    else:
        print('Please provide the filepath of the disaster messages database '
              'as the first argument and the filepath of the pickle file to '
              'save the model to as the second argument. \n\nExample: python '
              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')


if __name__ == '__main__':
    main()
